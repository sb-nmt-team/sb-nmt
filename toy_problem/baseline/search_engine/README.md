## Как пользоваться Search Engine и что тут вообще происходит:

0) Для нормальной работы нужен файл `se.bin`. Он лежит в [git lfs](https://git-lfs.github.com/).
Cоответственно, его надо не только спулить, но и проделать дополнителные махинации, что-то типа:

```
git lfs install
git lfs fetch
git lfs checkout
```

1) TLDR:
```
>>> from searchengine import SearchEngine
>>> se = SearchEngine()

>>> # This might take a while
>>> se.load("se.bin")

>>> se("'a 'a d y r")
[(0.8, ('l', "'a", 'd', 'y', 'r')), (0.8, ('w', "'a", 'd', 'y', 'r')), (0.8, ('y', "'a", 'd', 'y', 'r'))]

>>> se(("'a", "'a", 'd', 'y', 'r'))

[(0.8, ('l', "'a", 'd', 'y', 'r')), (0.8, ('w', "'a", 'd', 'y', 'r'))]
```


2) Длинная версия

  2.1) `se.bin`:

  В файле se.bin храниться сохраненный pickle-ом словарь вида {"result": result, "dataset": dataset}, 
  где dataset -- это список предложений из train корпуса, а resutl -- это словарь, 
  отображающий tuple предложения (типа ("'a", "'a", 'd', 'y', 'r')) в набор отсортированных пар (score, index), 
  где score -- это близость предложения из train set-a к нашему (float от 0 до 1),
  а index -- это индекс предложения в массиве dataset. Для каждого слова из train + dev + test хранится
  100 ближайших предложений
  
  2.2) `run.py`
   
  С помощью скрипта `run.py` был как раз предподсчитан файл se.bin.
  Насколько я помню, в 16 потоков это добро считалось где-то 2 часа.
  
  Внутри `run.py` есть следующие параметры:
  
  * TRAIN_SET_PATH -- путь до файла с предложеними, до которых будет считаться индекс (train_set)
    
  * DATASET_PATHS -- пути до файлов с предложениями, от которых будет считаться индекс (query_set)
    
  * N_NEIGHBOURS -- сколько ближайших предложений из train_set будет храниться
    
  * N_JOBS -- в сколько процессов все будет одновременно считаться
    
  * N_CHUNKS -- на столько частей разобьется весь query_set. Влияет в основном на частоту обновлений tqdm-bar'a
      не имеет смысла делать меньше, чем N_JOBS. В теории, чем N_CHUNKS больше, тем все медленнее, но,
      мне кажется, что не сильно (непроверенное утверждение)
   
  Для того, чтобы запустить `run.py` нужны модули `editdistance`, `numpy`, `tqdm`
  
  2.3) `searchengine.py`
  
  Интерфейс, для общения с кэшом. Через метод `.load(path)` загружается предподсчитаный файл `se.bin`, 
  через `.__call__(sentence, n_neighbours=None)` (то есть оператор круглые скобки)
  получаются ближайшие соседи для данного предложения.
  
  Предложение (`sentence`) -- это либо строка (тогда его побьют по пробельным символам на слова),
  либо любой другой iterable  строк.
  
  `n_neighbours` -- ограничение на то, сколько ближайших соседей возвращать (не больше, чем `N_NEIGHBOURS` при предподсчете).
  
  Если предложения не было в query_set, то для него ближайшие соседи посчитаются заново (это медленно).
  При этом вызовется метод `not_found_warning` (который сейчас ничего не делает, но его можно переопределить)
  
  
  2.4) `utils.py`
  
  Всякие функции, чтобы все вышеописанное работало. Можете почитать.
  Например, наверное интересно будет менять функцию `calucate_metric`
  
